---
layout: post
title:  "Runtimes"
date:   2018-06-15
permalink: 'runtimes.html'
---

 
_Runtimes_ are servers with the predefined infrasturcture. They must implement a set if specific methods that are used by `manager`. Those servers run in the docker environments and responsible for running user-defined models. When you create a new application and assign a model to it, you also have to provide a specific runtime, which will be used to run your model. 

# Available runtimes

We're already implemented a few runtimes which you can use in your own projects. They are all opensource and you can look up code if you need. 

| Runtime | Versions | Links
| ------- | ------- | ---- |
| hydrosphere/serving-runtime-python | 3.6-latest<br>3.5-latest<br>3.4-latest | [Docker Hub][docker-hub-python]<br>[Github][github-serving-python]|
| hydrosphere/serving-runtime-tensorflow | 1.4.0-latest<br>1.5.0-latest<br>1.6.0-latest<br>1.7.0-latest | [Docker Hub][docker-hub-tensorflow]<br>[Github][github-serving-tensorflow] |
| hydrosphere/serving-runtime-spark | 2.0-latest<br>2.1-latest<br>2.2-latest | [Docker Hub][docker-hub-spark]<br>[Github][github-serving-spark] |


# Developing runtime


Sometimes you have to use technology that we are not supporting yet or you need more flexibility and you want to implement your own runtime. It may seem frightening at first glance, but it's actually not that difficult. ML Lambda is designed to abstract it's guts from model users and runtime developers. The key things you have to know to write your own runtime are: 

* Knowing how to implement a predefined gRPC service for a dedicated language;
* Understanding our contracts' protobufs to describe entrypoints, such as inputs and outputs. (This requires knowledge about protobuf protocol);
* Knowing how to create your own docker image and publish it to an open registry.


## Example
### Generating GRPC code

There're different approaches on how to generate client and server gRPC code on [different languages][grpc-docs]. Let's have a look on how to do that on Python.

First, let's clone our [protocols][github-serving-protos] and prepare a folder for generated code.

```sh
$ git clone https://github.com/Hydrospheredata/hydro-serving-protos
$ mkdir runtime
```

For generating gRPC code we need additional packages. 

```sh
$ pip install grpcio-tools googleapis-common-protos
```

Runtimes only depend on `contracts` and `tf`, so we're generating only them.

```sh
$ python -m grpc_tools.protoc --proto_path=./hydro-serving-protos/src/ --python_out=./runtime/ --grpc_python_out=./runtime/ $(find ./hydro-serving-protos/src/hydro_serving_grpc/contract/ -type f -name '*.proto')
$ python -m grpc_tools.protoc --proto_path=./hydro-serving-protos/src/ --python_out=./runtime/ --grpc_python_out=./runtime/ $(find ./hydro-serving-protos/src/hydro_serving_grpc/tf/ -type f -name '*.proto')
```

For convinience we can also add `__init__.py` files to the generated directories. 

```sh
$ cd runtime
$ find ./hydro_serving_grpc -type d -exec touch {}/__init__.py \;
```

The structure of the `runtime` now should be as following:

```sh
runtime
└── hydro_serving_grpc
    ├── __init__.py
    ├── contract
    │   ├── __init__.py
    │   ├── model_contract_pb2.py
    │   ├── model_contract_pb2_grpc.py
    │   ├── model_field_pb2.py
    │   ├── model_field_pb2_grpc.py
    │   ├── model_signature_pb2.py
    │   └── model_signature_pb2_grpc.py
    └── tf
        ├── __init__.py
        ├── api
        │   ├── __init__.py
        │   ├── model_pb2.py
        │   ├── model_pb2_grpc.py
        │   ├── predict_pb2.py
        │   ├── predict_pb2_grpc.py
        │   ├── prediction_service_pb2.py
        │   └── prediction_service_pb2_grpc.py
        ├── tensor_pb2.py
        ├── tensor_pb2_grpc.py
        ├── tensor_shape_pb2.py
        ├── tensor_shape_pb2_grpc.py
        ├── types_pb2.py
        └── types_pb2_grpc.py
```

### Implementing Service

Now, that we have everything set up, let's actually implement runtime.

`runtime.py`
```python
from hydro_serving_grpc.tf.api.predict_pb2 import PredictRequest, PredictResponse
from hydro_serving_grpc.tf.api.prediction_service_pb2_grpc import PredictionServiceServicer, add_PredictionServiceServicer_to_server
from hydro_serving_grpc.tf.types_pb2 import *
from hydro_serving_grpc.tf.tensor_pb2 import TensorProto
from hydro_serving_grpc.contract.model_contract_pb2 import ModelContract
from concurrent import futures

import os
import time
import grpc
import logging
import importlib


class PythonRuntimeService(PredictionServiceServicer):
    def __init__(self, model_path, contract):
        self.logger = logging.getLogger(self.__class__.__name__)
        self.model_path = model_path
        self.contract = contract

    def Predict(self, request, context):
        self.logger.info(f"Received inference request: {request}")
        
        selected_signature = None
        for index, signature in enumerate(self.contract.signatures):
            if signature.signature_name == request.model_spec.signature_name:
                selected_signature = signature

        if selected_signature is None:
          context.set_code(grpc.StatusCode.INVALID_ARGUMENT)
          context.set_details(f'{request.model_spec.signature_name} is not present in the model')
          return PredictResponse()

        module = importlib.import_module("func_main")
        executable = getattr(module, selected_signature.signature_name)
        executable(**request.inputs)

        if not isinstance(result, hs.PredictResponse):
            self.logger.warning(f"Type of a result ({result}) is not `PredictResponse`")
            context.set_code(grpc.StatusCode.OUT_OF_RANGE)
            context.set_details(f"Type of a result ({result}) is not `PredictResponse`")
            return PredictResponse()

        return PredictResponse(outputs=outputs)


class PythonRuntimeManager:
    def __init__(self, model_path, port):
        self.logger = logging.getLogger(self.__class__.__name__)
        self.port = port
        self.model_path = model_path
        self.server = None
        
        with open(os.path.join(model_path, 'contract.protobin')) as file:
            contract = ModelContract.ParseFromString(file.read())
        self.servicer = PythonRuntimeService(os.path.join(self.model_path, 'files'), contract)

    def start(self):
        self.logger.info(f"Starting PythonRuntime at {self.port}")
        self.server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))
        add_PredictionServiceServicer_to_server(self.servicer, self.server)
        self.server.add_insecure_port(f'[::]:{self.port}')
        self.server.start()

    def stop(self, code=0):
        self.logger.info(f"Stopping PythonRuntime at {self.port}")
        self.server.stop(code)
```

Let's quickly review, what we have here. `PythonRuntimeManager` simply manages our service, i.e. starts it, stops it, holds all necessary data. `PythonRuntimeService` is our service, that actually implements `Predict(PredictRequest)`.

The model is stored inside the `/model` directory. The structure of `/model`: 

```sh
model
├── contract.protobin
└── files
    ├── ...
    └── ...
```

`contract.protobin` is a processed by `manager` binary representation of the [ModelContract][github-contract-proto] message. `files` directory contains all files of your model.

To run your service let's create another file.

`main.py`
```python
from runtime import PythonRuntimeManager

import os
import time
import logging

logging.basicConfig(level=logging.INFO)

if __name__ == '__main__':
    runtime = PythonRuntimeManager('/model', port=int(os.getenv('APP_PORT', "9090")))
    runtime.start()

    try:
        while True:
            # Since after starting the server the execution will be continued, 
            # we do not want to shut the program down, so we put main thread on hold
            time.sleep(60 * 60 * 24)

    except KeyboardInterrupt:
        runtime.stop()
```

### Publishing Runtime

Before we can use the runtime, we have to wrap it into docker image.

Add requirements, so we can easily setup all dependencies.

`requirements.txt`
```
grpcio==1.12.1 
googleapis-common-protos==1.5.3 
``` 
Add Dockerfile to wrap up our image.

`Dockerfile`
```docker
FROM python:3.6.5 

ENV APP_PORT=9090
LABEL DEPLOYMENT_TYPE=APP

RUN pip install -r requirements.txt

VOLUME /model 
ADD . /app
WORKDIR /app

CMD ["python", "python_runtime.py"]
```

Now we can build our image and publish it to the Docker Hub.
```sh
$ docker build -t {username}/python-runtime-example:3.6.5
$ docker push {username}/python-runtime-example:3.6.5 
```

The `username` should be the one you have registered in Docker Hub. Building and pushing image might take a while depending on your internet connection. If you're working with a local instance of ML Lambda, you don't actually have to publish your image, manager can get it from your local images. 

### Fetching Runtime

To add your newly created runtime just execute the following lines. Note, your ML Lambda instance should be up.

```sh
$ curl -X POST --header 'Content-Type: application/json' --header 'Accept: application/json' -d '{
   "name": "username/python-runtime-example",
   "version": "3.6.5",
   "modelTypes": [
     "string"
   ],
   "tags": [
     "string"
   ],
   "configParams": {}
 }' 'http://localhost:8080/api/v1/runtime'
```

Here, `name` is your image published in the Docker Hub. `version` is the tag of that image. 

### Result

That's it. You just created a sample runtime, that you can use in your own projects. This is a slightly shortened example of our [python runtime implementation][github-python-runtime]. You can always look up details there. 



[grpc-docs]: https://grpc.io/docs/
[docker-hub]: https://hub.docker.com/u/hydrosphere/
[docker-hub-python]: https://hub.docker.com/r/hydrosphere/serving-runtime-python/
[docker-hub-spark]: https://hub.docker.com/r/hydrosphere/serving-runtime-spark/
[docker-hub-tensorflow]: https://hub.docker.com/r/hydrosphere/serving-runtime-tensorflow/
[docker-hub-scikit]: https://hub.docker.com/r/hydrosphere/serving-runtime-scikit/
[docker-hub-pytorch]: https://hub.docker.com/r/hydrosphere/serving-runtime-pytorch/
[github-serving-python]: https://github.com/Hydrospheredata/hydro-serving-python
[github-serving-tensorflow]: https://github.com/Hydrospheredata/hydro-serving-tensorflow
[github-serving-spark]: https://github.com/Hydrospheredata/hydro-serving-spark
[github-serving-protos]: https://github.com/Hydrospheredata/hydro-serving-protos
[github-contract-proto]: https://github.com/Hydrospheredata/hydro-serving-protos/blob/master/src/hydro_serving_grpc/contract/model_contract.proto
[github-service-proto]: https://github.com/Hydrospheredata/hydro-serving-protos/blob/master/src/hydro_serving_grpc/tf/api/prediction_service.proto
[github-python-runtime]: https://github.com/Hydrospheredata/hydro-serving-python